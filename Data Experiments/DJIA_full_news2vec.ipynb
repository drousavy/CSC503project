{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out these lines if you have already installed these modules\n",
    "\n",
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "DJIA = pd.read_csv(r\"Combined_News_DJIA.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = DJIA[DJIA.columns[1:26]]\n",
    "\n",
    "\n",
    "cols = df.columns[1:25]\n",
    "\n",
    "\n",
    "df['combined'] = df[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['combined'], df['Label'], test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# function that cleans the News article\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens =[]\n",
    "    for i, News in doc.iterrows():\n",
    "            token = doc.loc[i,'combined']\n",
    "            token = token.split()\n",
    "            tokens = token+tokens\n",
    "        #remove punctuation from each token\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            tokens = [w.translate(table) for w in tokens]\n",
    "             # remove remaining tokens that are not alphabetic\n",
    "            tokens = [word for word in tokens if word.isalpha()]\n",
    "             # filter out stop words\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = [w for w in tokens if not w in stop_words]\n",
    "            # filter out short tokens\n",
    "            tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.to_frame().reset_index()\n",
    "y_train = y_train.to_frame().reset_index().apply(lambda x: np.sign(x))\n",
    "X_test = X_test.to_frame().reset_index()\n",
    "y_test = y_test.to_frame().reset_index().apply(lambda x: np.sign(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokens = clean_doc(X_train)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "vocab = Counter()\n",
    "vocab.update(tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "y_train = y_train['Label'].to_numpy()\n",
    "y_test = y_test['Label'].to_numpy()    \n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network function that I used on the previous dataset. I commented it out since this script is just supposed\n",
    "#to turn the news articles into vectors and provide the associated labels.\n",
    "\n",
    "# def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "#     scores = list()\n",
    "#     n_repeats = 10\n",
    "#     n_words = Xtest.shape[1]\n",
    "#     for i in range(n_repeats):\n",
    "#         # define network\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "#         model.add(Dense(1, activation='sigmoid'))\n",
    "#         # compile network\n",
    "#         model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#         # fit network\n",
    "#         model.fit(Xtrain, ytrain, epochs=50, verbose=2)\n",
    "#         # evaluate\n",
    "#         loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "#         scores.append(acc)\n",
    "#         print('%d accuracy: %s' % ((i + 1), acc))\n",
    "#     return scores\n",
    "\n",
    "\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode training data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "\n",
    "mode = 'binary'\n",
    "\n",
    "results = DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Xtrain, Xtest = prepare_data(X_train['combined'], X_test['combined'], mode)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
