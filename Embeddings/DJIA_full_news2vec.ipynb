{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out these lines if you have already installed these modules\n",
    "\n",
    "#!pip install nltk\n",
    "#!pip install tensorflow\n",
    "#!pip install keras\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nimmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news(news):\n",
    "    _news = news.replace('b\\\"', \"\")\n",
    "    _news = _news.replace('b\\'', \"\")\n",
    "    _news = _news.lower()\n",
    "    _news = re.sub(\"[^a-zA-Z]\", \" \",_news)\n",
    "    _news = re.sub('[\\s]+', ' ', _news)\n",
    "    \n",
    "    tokens = _news.split(\" \")\n",
    "    if \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    #remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    _news = ' '.join(tokens)    \n",
    "     \n",
    "    return _news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "\n",
    "    data = pd.read_csv(\"../Datasets/djia/Combined_News_DJIA.csv\")\n",
    "    \n",
    "    dfs = []\n",
    "    data[\"News\"] = \"\"\n",
    "    for i in range(1,25):\n",
    "        col = \"Top\"+str(i)\n",
    "        data[\"News\"] = data[\"News\"] +\" \"+ data[col]\n",
    "    data = data.dropna()\n",
    "    data['PreProcessedNews'] = data['News'].map(process_news)\n",
    "    \n",
    "    data = data[['Date', 'News', 'PreProcessedNews', 'Label']]\n",
    "    \n",
    "    stock_prices = \"../Datasets/djia/upload_DJIA_table.csv\"\n",
    "    stock_data = pd.read_csv(stock_prices)\n",
    "    \n",
    "    print(data.head(2))\n",
    "    print(stock_data.head(2))\n",
    "    \n",
    "    \n",
    "    #merged_dataframe = data.merge(stock_data, how='inner', on='Date')\n",
    "    merged_dataframe = pd.merge(data, stock_data, how='inner', on = 'Date')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    Xy_train = merged_dataframe[:int(len(data)*0.6)]\n",
    "    Xy_valid = merged_dataframe[int(len(data)*0.6):int(len(data)*0.8)]\n",
    "    Xy_test = merged_dataframe[int(len(data)*0.8):]\n",
    "    \n",
    "    return merged_dataframe, Xy_train, Xy_valid, Xy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date                                               News  \\\n",
      "0  2008-08-08   b\"Georgia 'downs two Russian warplanes' as co...   \n",
      "1  2008-08-11   b'Why wont America and Nato help us? If they ...   \n",
      "\n",
      "                                    PreProcessedNews  Label  \n",
      "0  georgia two russian warplane country move brin...      0  \n",
      "1  wont america nato help wont help help iraq bus...      1  \n",
      "         Date          Open          High           Low         Close  \\\n",
      "0  2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   \n",
      "1  2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234   \n",
      "\n",
      "      Volume     Adj Close  \n",
      "0   82160000  17949.369141  \n",
      "1  133030000  17929.990234  \n"
     ]
    }
   ],
   "source": [
    "news, Xy_train, Xy_valid, Xy_test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Xy_train['PreProcessedNews']\n",
    "X_valid = Xy_valid['PreProcessedNews']\n",
    "X_test = Xy_test['PreProcessedNews']\n",
    "y_train = Xy_train['Label'].to_numpy()\n",
    "y_valid = Xy_valid['Label'].to_numpy()\n",
    "y_test = Xy_test['Label'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_docs, valid_docs, test_docs, mode):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode training data set\n",
    "    Xvalid = tokenizer.texts_to_matrix(valid_docs, mode=mode)\n",
    "    # encode testing data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xvalid, Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vectors for each sentence\n",
    "* binary\n",
    "* count\n",
    "* tfidf\n",
    "* freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "\n",
    "mode = 'binary'\n",
    "\n",
    "X_train, X_valid, X_test = prepare_data(X_train, X_valid, X_test, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 22641)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1191, 22641)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dimentionality since this is a sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=50, n_iter=10)\n",
    "X_train = svd.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = svd.transform(X_valid)\n",
    "X_test = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1191, 50)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 50)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply machine learning example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-78a0771378b9>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-78a0771378b9>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    bla bla bla\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
